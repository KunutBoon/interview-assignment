{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Prediction Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since model prediction workflows are mainly involved with well-structured, clean, and modularized codes, all of the codes from previous experimental phase (credit_default_risk_experiment.ipynb) are organized and broken down into each sub-function. \n",
    "\n",
    "Those functions are also grouped into modules (.py file) and only main function, which is typically a sequence of sub-function, is called in each step to achieve the task.\n",
    "\n",
    "Moreover, the SKlean Pipeline is also used at this stage to assemble all estimator, such as, MinMaxScaler, OneHotEncoder, together as a one big chain to sequentially apply a list of transformers to the data at once.\n",
    "\n",
    "All of well-structured codes are contained inside each .py file in the following structure :\n",
    "\n",
    "```\n",
    "ROOT\n",
    "├── src\n",
    "│   ├── utils.py\n",
    "│   ├── data_preprocessing.py\n",
    "│   ├── model_training.py\n",
    "│   └── model_evaluation.py\n",
    "├── pipeline_train.py **\n",
    "└── pipeline_inference.py **\n",
    "```\n",
    "\n",
    "where all .py files in <code>src</code> folder contain all the sub-functions and all the files outside particular folder are the main files to call for executing the Training and Inference Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "dir_project_root = os.path.dirname(os.getcwd())\n",
    "sys.path.append(dir_project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned, the Training Pipeline was designed as the main function which call sub-functions from those modules inside <code>src</code> folder.\n",
    "\n",
    "\n",
    "Training Pipeline is mainly involved with 4 steps, which are including :\n",
    " - Data Preprocessing step - preprocess raw data into a format which suitable for model training\n",
    "   <p><em>function : <code>preprocess_step()</code></em></p>\n",
    " - Model Training step - train the SKLearn Pipeline object using data from previous steps, together with configurations found from experiment phase\n",
    "   <p><em>function : <code>model_training_step()</code></em></p>\n",
    " - Model Evaluation step - apply the prediction on hold-out testing data and evaluate the performance (AUC score)\n",
    "   <p><em>function : <code>model_evaluation_step()</code></em></p>\n",
    " - Inference Config step - the purpose of this step is mainly to document the latest version of trained model as a referece for Inference Pipeline while making prediction\n",
    "    <p><em>function : <code>inference_config()</code></em></p>\n",
    "\n",
    "The logging was also included to record the progress of pipeline execution along with some importance information, such as model accuracy.\n",
    "\n",
    "In order to execute the pipeline, you can only use the magic command given in next cell to execte the file directly. After that, all the Training process will be executed automatically to create all of those artifacts in the directory.\n",
    "\n",
    "<em><u>NOTE</u> : If you want to test the execution, please only execute the file in this Jupyter notebook using the given command below since there is the robustness issue on the logic being used to find root directory. This issue might cause execution error if you execute the file directly from elsewhere.</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 02:24:10 - __main__ - INFO - main:287 - Start training pipeline\n",
      "2024-02-08 02:24:10 - __main__ - DEBUG - preprocess_step:65 - STEP PROCESSING\n",
      "2024-02-08 02:26:32 - __main__ - DEBUG - preprocess_step:114 - Training features shape --> (276759, 64), Training target shape --> (276759,)\n",
      "2024-02-08 02:26:32 - __main__ - DEBUG - preprocess_step:115 - Evaluation features shape --> (30752, 64), Evaluation target shape --> (30752,)\n",
      "2024-02-08 02:26:41 - __main__ - DEBUG - save_csv_file:153 - Csv file saved at c:\\Users\\11413929\\repos\\int_ass\\data\\production\\process_data\\train\\features_train_2024_02_08_02_26_32.csv\n",
      "2024-02-08 02:26:42 - __main__ - DEBUG - save_csv_file:153 - Csv file saved at c:\\Users\\11413929\\repos\\int_ass\\data\\production\\process_data\\test\\features_test_2024_02_08_02_26_32.csv\n",
      "2024-02-08 02:26:42 - __main__ - DEBUG - save_ndarray:189 - Numpy array file saved at c:\\Users\\11413929\\repos\\int_ass\\data\\production\\process_data\\train\\target_train_2024_02_08_02_26_32.npy\n",
      "2024-02-08 02:26:42 - __main__ - DEBUG - save_ndarray:189 - Numpy array file saved at c:\\Users\\11413929\\repos\\int_ass\\data\\production\\process_data\\test\\target_test_2024_02_08_02_26_32.npy\n",
      "2024-02-08 02:26:42 - __main__ - DEBUG - save_list:164 - List file saved at c:\\Users\\11413929\\repos\\int_ass\\data\\production\\artifacts\\list_col_numeric_2024_02_08_02_26_32.sav\n",
      "2024-02-08 02:26:42 - __main__ - DEBUG - save_list:164 - List file saved at c:\\Users\\11413929\\repos\\int_ass\\data\\production\\artifacts\\list_col_nominal_2024_02_08_02_26_32.sav\n",
      "2024-02-08 02:26:42 - __main__ - DEBUG - save_list:164 - List file saved at c:\\Users\\11413929\\repos\\int_ass\\data\\production\\artifacts\\list_col_high_corr_2024_02_08_02_26_32.sav\n",
      "2024-02-08 02:26:42 - __main__ - DEBUG - save_list:164 - List file saved at c:\\Users\\11413929\\repos\\int_ass\\data\\production\\artifacts\\list_col_heavy_nan_2024_02_08_02_26_32.sav\n",
      "2024-02-08 02:26:42 - __main__ - DEBUG - save_dict:176 - Dict file saved at c:\\Users\\11413929\\repos\\int_ass\\data\\production\\artifacts\\dict_col_selected_cat_2024_02_08_02_26_32.json\n",
      "2024-02-08 02:26:42 - __main__ - DEBUG - model_training_step:169 - STEP TRAINING\n",
      "2024-02-08 02:26:44 - __main__ - DEBUG - load_csv_file:158 - Csv file loaded from c:\\Users\\11413929\\repos\\int_ass\\data\\production\\process_data\\train\\features_train_2024_02_08_02_26_32.csv\n",
      "2024-02-08 02:26:44 - __main__ - DEBUG - load_ndarray:195 - Numpy array file loaded from c:\\Users\\11413929\\repos\\int_ass\\data\\production\\process_data\\train\\target_train_2024_02_08_02_26_32.npy\n",
      "2024-02-08 02:26:44 - __main__ - DEBUG - load_dict:182 - Dict file loaded from c:\\Users\\11413929\\repos\\int_ass\\data\\production\\artifacts\\dict_col_selected_cat_2024_02_08_02_26_32.json\n",
      "2024-02-08 02:26:44 - __main__ - DEBUG - load_list:169 - List file loaded from c:\\Users\\11413929\\repos\\int_ass\\data\\production\\artifacts\\list_col_numeric_2024_02_08_02_26_32.sav\n",
      "2024-02-08 02:26:44 - __main__ - DEBUG - load_list:169 - List file loaded from c:\\Users\\11413929\\repos\\int_ass\\data\\production\\artifacts\\list_col_nominal_2024_02_08_02_26_32.sav\n",
      "2024-02-08 02:26:44 - __main__ - DEBUG - load_dict:182 - Dict file loaded from c:\\Users\\11413929\\repos\\int_ass\\config\\training\\hyper_param_config.json\n",
      "c:\\Users\\11413929\\AppData\\Local\\miniconda3\\envs\\r_atm_poc\\lib\\site-packages\\xgboost\\data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "2024-02-08 02:26:57 - __main__ - DEBUG - model_training_step:208 - Training AUC score = 0.7913\n",
      "2024-02-08 02:26:57 - __main__ - DEBUG - save_model:201 - Sklearn model obj saved at c:\\Users\\11413929\\repos\\int_ass\\model_pipeline\\model_pipeline_2024_02_08_02_26_32.sav\n",
      "2024-02-08 02:26:57 - __main__ - DEBUG - model_evaluation_step:230 - STEP EVALUATE\n",
      "2024-02-08 02:26:57 - __main__ - DEBUG - load_csv_file:158 - Csv file loaded from c:\\Users\\11413929\\repos\\int_ass\\data\\production\\process_data\\test\\features_test_2024_02_08_02_26_32.csv\n",
      "2024-02-08 02:26:57 - __main__ - DEBUG - load_ndarray:195 - Numpy array file loaded from c:\\Users\\11413929\\repos\\int_ass\\data\\production\\process_data\\test\\target_test_2024_02_08_02_26_32.npy\n",
      "2024-02-08 02:26:57 - __main__ - DEBUG - load_model:206 - Sklearn model obj loaded from c:\\Users\\11413929\\repos\\int_ass\\model_pipeline\\model_pipeline_2024_02_08_02_26_32.sav\n",
      "2024-02-08 02:26:57 - __main__ - DEBUG - load_list:169 - List file loaded from c:\\Users\\11413929\\repos\\int_ass\\data\\production\\artifacts\\list_col_nominal_2024_02_08_02_26_32.sav\n",
      "2024-02-08 02:26:57 - Training Version - INFO - model_evaluation_step:263 - Training version \"2024_02_08_02_26_32\", AUC score = 0.7583\n",
      "2024-02-08 02:26:57 - __main__ - DEBUG - model_evaluation_step:265 - Evaluation AUC score = 0.7583\n",
      "2024-02-08 02:26:57 - __main__ - DEBUG - model_evaluation_step:267 - Confusion Matrix : \n",
      "     [[19970  8299]\n",
      "      [  793  1690]]\n",
      "2024-02-08 02:26:57 - __main__ - DEBUG - model_evaluation_step:268 - Accuracy Score : 0.70\n",
      "2024-02-08 02:26:57 - __main__ - DEBUG - model_evaluation_step:269 - Precision Score : 0.17\n",
      "2024-02-08 02:26:57 - __main__ - DEBUG - model_evaluation_step:270 - Recall Score : 0.68\n",
      "2024-02-08 02:26:57 - __main__ - DEBUG - model_evaluation_step:271 - F1 Score : 0.27\n",
      "2024-02-08 02:26:57 - __main__ - DEBUG - inference_config:275 - WRITE CONFIG\n",
      "2024-02-08 02:26:57 - __main__ - DEBUG - save_dict:176 - Dict file saved at c:\\Users\\11413929\\repos\\int_ass\\config\\inference\\inference_config.json\n",
      "2024-02-08 02:26:57 - __main__ - INFO - main:301 - Completed training pipeline\n"
     ]
    }
   ],
   "source": [
    "%run {dir_project_root}\\pipeline_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference Pipeline was also designed as the main function which call sub-functions from those modules inside <code>src</code> folder.\n",
    "\n",
    "\n",
    "Training Pipeline is mainly involved with 3 steps, which are including :\n",
    " - Read Config step - read the configs which are previously documented by Training Pipeline\n",
    "   <p><em>function : <code>read_training_cfg()</code></em></p>\n",
    " - Data Preprocessing step - preprocess raw data into a same format which used while training model in order to make a prediction\n",
    "   <p><em>function : <code>model_training_step()</code></em></p>\n",
    " - Model Inference step - predict on unseen dataset as both scores (probability) and labels using given probability threshold\n",
    "   <p><em>function : <code>inference_step()</code></em></p>\n",
    "\n",
    "The logging was also included to record the progress of pipeline execution along with some importance information.\n",
    "\n",
    "In order to execute the pipeline, you can only use the magic command given in next cell to execte the file directly. After that, all the Inference process will be executed automatically to create prediction file in the directory.\n",
    "\n",
    "<em><u>NOTE</u> : If you want to test the execution, please only execute the file in this Jupyter notebook using the given command below since there is the robustness issue on the logic being used to find root directory. This issue might cause execution error if you execute the file directly from elsewhere.</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 02:27:50 - __main__ - INFO - main:153 - Start inference pipeline\n",
      "2024-02-08 02:27:50 - __main__ - INFO - main:153 - Start inference pipeline\n",
      "2024-02-08 02:27:50 - __main__ - DEBUG - load_dict:182 - Dict file loaded from c:\\Users\\11413929\\repos\\int_ass\\config\\inference\\inference_config.json\n",
      "2024-02-08 02:27:50 - __main__ - DEBUG - load_dict:182 - Dict file loaded from c:\\Users\\11413929\\repos\\int_ass\\config\\inference\\inference_config.json\n",
      "2024-02-08 02:27:50 - __main__ - DEBUG - preprocess_step:58 - STEP PROCESSING\n",
      "2024-02-08 02:27:50 - __main__ - DEBUG - preprocess_step:58 - STEP PROCESSING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 02:28:17 - __main__ - DEBUG - load_list:169 - List file loaded from c:\\Users\\11413929\\repos\\int_ass\\data\\production\\artifacts\\list_col_high_corr_2024_02_08_02_26_32.sav\n",
      "2024-02-08 02:28:17 - __main__ - DEBUG - load_list:169 - List file loaded from c:\\Users\\11413929\\repos\\int_ass\\data\\production\\artifacts\\list_col_high_corr_2024_02_08_02_26_32.sav\n",
      "2024-02-08 02:28:17 - __main__ - DEBUG - load_list:169 - List file loaded from c:\\Users\\11413929\\repos\\int_ass\\data\\production\\artifacts\\list_col_heavy_nan_2024_02_08_02_26_32.sav\n",
      "2024-02-08 02:28:17 - __main__ - DEBUG - load_list:169 - List file loaded from c:\\Users\\11413929\\repos\\int_ass\\data\\production\\artifacts\\list_col_heavy_nan_2024_02_08_02_26_32.sav\n",
      "2024-02-08 02:28:17 - __main__ - DEBUG - load_list:169 - List file loaded from c:\\Users\\11413929\\repos\\int_ass\\data\\production\\artifacts\\list_col_nominal_2024_02_08_02_26_32.sav\n",
      "2024-02-08 02:28:17 - __main__ - DEBUG - load_list:169 - List file loaded from c:\\Users\\11413929\\repos\\int_ass\\data\\production\\artifacts\\list_col_nominal_2024_02_08_02_26_32.sav\n",
      "2024-02-08 02:28:17 - __main__ - DEBUG - load_list:169 - List file loaded from c:\\Users\\11413929\\repos\\int_ass\\data\\production\\artifacts\\list_col_numeric_2024_02_08_02_26_32.sav\n",
      "2024-02-08 02:28:17 - __main__ - DEBUG - load_list:169 - List file loaded from c:\\Users\\11413929\\repos\\int_ass\\data\\production\\artifacts\\list_col_numeric_2024_02_08_02_26_32.sav\n",
      "2024-02-08 02:28:47 - __main__ - DEBUG - preprocess_step:100 - Inference features shape --> (48744, 64)\n",
      "2024-02-08 02:28:47 - __main__ - DEBUG - preprocess_step:100 - Inference features shape --> (48744, 64)\n",
      "2024-02-08 02:28:48 - __main__ - DEBUG - save_csv_file:153 - Csv file saved at c:\\Users\\11413929\\repos\\int_ass\\data\\production\\process_data\\inference\\features_predict_2024_02_08_02_28_47.csv\n",
      "2024-02-08 02:28:48 - __main__ - DEBUG - save_csv_file:153 - Csv file saved at c:\\Users\\11413929\\repos\\int_ass\\data\\production\\process_data\\inference\\features_predict_2024_02_08_02_28_47.csv\n",
      "2024-02-08 02:28:49 - __main__ - DEBUG - inference_step:121 - STEP TRAINING\n",
      "2024-02-08 02:28:49 - __main__ - DEBUG - inference_step:121 - STEP TRAINING\n",
      "2024-02-08 02:28:49 - __main__ - DEBUG - load_csv_file:158 - Csv file loaded from c:\\Users\\11413929\\repos\\int_ass\\data\\production\\process_data\\inference\\features_predict_2024_02_08_02_28_47.csv\n",
      "2024-02-08 02:28:49 - __main__ - DEBUG - load_csv_file:158 - Csv file loaded from c:\\Users\\11413929\\repos\\int_ass\\data\\production\\process_data\\inference\\features_predict_2024_02_08_02_28_47.csv\n",
      "2024-02-08 02:28:49 - __main__ - DEBUG - load_model:206 - Sklearn model obj loaded from c:\\Users\\11413929\\repos\\int_ass\\model_pipeline\\model_pipeline_2024_02_08_02_26_32.sav\n",
      "2024-02-08 02:28:49 - __main__ - DEBUG - load_model:206 - Sklearn model obj loaded from c:\\Users\\11413929\\repos\\int_ass\\model_pipeline\\model_pipeline_2024_02_08_02_26_32.sav\n",
      "2024-02-08 02:28:49 - __main__ - DEBUG - load_list:169 - List file loaded from c:\\Users\\11413929\\repos\\int_ass\\data\\production\\artifacts\\list_col_nominal_2024_02_08_02_26_32.sav\n",
      "2024-02-08 02:28:49 - __main__ - DEBUG - load_list:169 - List file loaded from c:\\Users\\11413929\\repos\\int_ass\\data\\production\\artifacts\\list_col_nominal_2024_02_08_02_26_32.sav\n",
      "2024-02-08 02:28:49 - __main__ - DEBUG - save_csv_file:153 - Csv file saved at c:\\Users\\11413929\\repos\\int_ass\\data\\production\\inference_output\\prediction_2024_02_08_02_28_47.csv\n",
      "2024-02-08 02:28:49 - __main__ - DEBUG - save_csv_file:153 - Csv file saved at c:\\Users\\11413929\\repos\\int_ass\\data\\production\\inference_output\\prediction_2024_02_08_02_28_47.csv\n",
      "2024-02-08 02:28:49 - __main__ - INFO - main:165 - Completed inference pipeline\n",
      "2024-02-08 02:28:49 - __main__ - INFO - main:165 - Completed inference pipeline\n"
     ]
    }
   ],
   "source": [
    "%run {dir_project_root}\\pipeline_inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the exection was completed, we can inspect the prediction results using the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_latest_file_dir, load_csv_file, DIR_INFERENCE_OUTPUT\n",
    "import pandas as pd\n",
    "\n",
    "last_prediction_file_name = get_latest_file_dir(DIR_INFERENCE_OUTPUT, file_type='.csv')\n",
    "last_prediction_file_path = os.path.join(DIR_INFERENCE_OUTPUT, last_prediction_file_name)\n",
    "df_prediction = pd.read_csv(last_prediction_file_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last prediction directory: c:\\Users\\11413929\\repos\\int_ass\\data\\production\\inference_output\\prediction_2024_02_08_02_28_47.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100001</th>\n",
       "      <td>0</td>\n",
       "      <td>0.354832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100005</th>\n",
       "      <td>1</td>\n",
       "      <td>0.622490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100013</th>\n",
       "      <td>0</td>\n",
       "      <td>0.191359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100028</th>\n",
       "      <td>1</td>\n",
       "      <td>0.425296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100038</th>\n",
       "      <td>1</td>\n",
       "      <td>0.633031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100042</th>\n",
       "      <td>0</td>\n",
       "      <td>0.234196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pred      prob\n",
       "SK_ID_CURR                \n",
       "100001         0  0.354832\n",
       "100005         1  0.622490\n",
       "100013         0  0.191359\n",
       "100028         1  0.425296\n",
       "100038         1  0.633031\n",
       "100042         0  0.234196"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Last prediction directory: {}\".format(last_prediction_file_path))\n",
    "df_prediction.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r_atm_poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
